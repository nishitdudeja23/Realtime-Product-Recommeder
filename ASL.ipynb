{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType\n",
    "\n",
    "spark=SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ML\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/\") \\\n",
    "    .config(\"spark.driver.memory\",\"12g\")\\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", \"[{'$sample': {'size': 1000000} }]\")\\\n",
    "    .option(\"uri\", \"mongodb://localhost:27017/\"+\"amazon\"+\".\"+\"data\")\\\n",
    "    .option(\"partitioner\", \"MongoSinglePartitioner\") \\\n",
    "    .option(\"partitionkey\", \"asin\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+\n",
      "|      asin|overall|    reviewerID|\n",
      "+----------+-------+--------------+\n",
      "|0912006684|    4.0|A2SAI3ADSWEBYQ|\n",
      "|1441766073|    4.0|A3KUEM2NAPZF61|\n",
      "|B015AUYDS4|    5.0| AAG4HS8SPN8B4|\n",
      "|B00JKOMISY|    3.0|A137UV8Z648ZM4|\n",
      "|1613757409|    5.0|A3QG1VOMYJTO6T|\n",
      "|B00M7QSHCS|    5.0| AIWZZYQMVMF6Z|\n",
      "|B00WDXQO66|    5.0|A2GTSFEAHPY4EL|\n",
      "|0385344422|    5.0|A20Z4OSXY7JATM|\n",
      "|B002AB7X6Q|    5.0|A1QFXFZM9Z4V8N|\n",
      "|0007350899|    5.0| ASBC6R8ZWO46O|\n",
      "|B011UK0P3A|    5.0| AOOFGDPE1DLA6|\n",
      "|1565124766|    5.0|A16R0NTMJA7O6Q|\n",
      "|0985911077|    4.0|A1S328ADAE6BAS|\n",
      "|1530850649|    4.0|A1J9D2BSI6AH5D|\n",
      "|B00005MM0K|    5.0| ADJXL0W0RJ7VV|\n",
      "|1441310584|    4.0|A2ALTWSLTVJ553|\n",
      "|B01GL8CI8Q|    3.0|A1BO3NH4KKASUT|\n",
      "|0800719980|    5.0| AQX3IAMUNZ2L9|\n",
      "|1568364784|    5.0|A28FFHMLZMCKM1|\n",
      "|B0076DI6EG|    5.0|A2NTP6U0F38U2B|\n",
      "+----------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('_id','reviewText','reviewerName','summary','unixReviewTime','verified')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+----------+----------------+\n",
      "|      asin|overall|    reviewerID|asin_index|reviewerID_index|\n",
      "+----------+-------+--------------+----------+----------------+\n",
      "|0743246136|    4.0| ASIU59L8NHMC3|  595779.0|        910631.0|\n",
      "|B00KO4518I|    5.0|A1MIFEZ821SZ4Z|   11336.0|        910631.0|\n",
      "|B018EUJLAW|    5.0|A3KWUH5MNCU7YO|  559060.0|        640075.0|\n",
      "|1932458328|    5.0|  AN733OOJ7WDB|  595779.0|        910631.0|\n",
      "|B0001FI51U|    5.0|A1XPBG9RU276SZ|  595779.0|        910631.0|\n",
      "|0679405127|    4.0|A3L5BQIQHKQ84C|  595779.0|        910631.0|\n",
      "|B002BG0SO4|    5.0| ANR071R0BJH1M|  331468.0|        910631.0|\n",
      "|B01A01X154|    1.0|A1TEGFXSN7FMUR|  595779.0|        243861.0|\n",
      "|B00006IBFA|    5.0| AL5TTPDVUC81W|   49001.0|        818085.0|\n",
      "|B005T43EAU|    2.0|A3LDVURZL4VJL8|  595779.0|        910631.0|\n",
      "|0825305888|    1.0|A1UB8YE85TKSP5|  186398.0|        910631.0|\n",
      "|B000FMNWRQ|    5.0|A3QQVDTQ9MAYTR|  595779.0|        910631.0|\n",
      "|B0002H3ZLM|    5.0|A3ONWERRPKZ3HI|    2063.0|        910631.0|\n",
      "|B00L9J9AZ2|    1.0|A16T8PV3XNGCOA|   61816.0|        910631.0|\n",
      "|B01C4A5PHQ|    5.0|A1E3P76KDGCEEA|  595779.0|        910631.0|\n",
      "|0787983705|    3.0|A2I1ESZ3PQ5H0Q|  595779.0|        910631.0|\n",
      "|0978573293|    4.0| AM62Y35QG4UVJ|  194024.0|        910631.0|\n",
      "|0345381432|    4.0|A1LFR7V8NXIE3S|  595779.0|        910631.0|\n",
      "|B001ALWVQ4|    5.0|A1DX578XX2FRD8|  595779.0|        910631.0|\n",
      "|B007CSF3IW|    5.0|A2OXEWITJRLXKU|  595779.0|        910631.0|\n",
      "+----------+-------+--------------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(\n",
    "    inputCols=['asin', 'reviewerID'],\n",
    "    outputCols=['asin_index', 'reviewerID_index'],\n",
    "    handleInvalid='keep'\n",
    ")\n",
    "final_df = indexer.fit(df).transform(df)\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on 'asin_indexed' and 'reviewerID_indexed' columns\n",
    "final_df = final_df.dropDuplicates(['asin_index', 'reviewerID_index'])\n",
    "\n",
    "# Drop NaN values\n",
    "final_df = final_df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop('reviewerID','asin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------+\n",
      "|overall|asin_index|reviewerID_index|\n",
      "+-------+----------+----------------+\n",
      "|    5.0|       0.0|           705.0|\n",
      "|    5.0|       0.0|          1899.0|\n",
      "|    4.0|       0.0|          2963.0|\n",
      "|    4.0|       0.0|         11383.0|\n",
      "|    4.0|       0.0|         31074.0|\n",
      "|    2.0|       0.0|         40235.0|\n",
      "|    5.0|       0.0|        107757.0|\n",
      "|    4.0|       0.0|        147599.0|\n",
      "|    3.0|       0.0|        213614.0|\n",
      "|    5.0|       0.0|        250967.0|\n",
      "|    5.0|       0.0|        267808.0|\n",
      "|    5.0|       0.0|        268444.0|\n",
      "|    4.0|       0.0|        288227.0|\n",
      "|    2.0|       0.0|        291214.0|\n",
      "|    4.0|       0.0|        298574.0|\n",
      "|    3.0|       0.0|        306687.0|\n",
      "|    4.0|       0.0|        307641.0|\n",
      "|    5.0|       0.0|        322971.0|\n",
      "|    5.0|       0.0|        353635.0|\n",
      "|    2.0|       0.0|        360187.0|\n",
      "+-------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_maj=final_df[final_df['overall']==5.0]\n",
    "df_min = final_df[final_df['overall']!=5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|overall| count|\n",
      "+-------+------+\n",
      "|    5.0|201126|\n",
      "|    4.0| 97230|\n",
      "|    3.0| 43214|\n",
      "|    1.0| 34609|\n",
      "|    2.0| 24298|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "upsample_df = df_min.sample(True, float(df_maj.count())/float(df_min.count()), seed=50)\n",
    "upsample_df = upsample_df.union(df_maj)\n",
    "upsample_df.groupBy(\"overall\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = upsample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_df.randomSplit([0.70,0.20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Train the ALS model on the training set\n",
    "als_mf = ALS(maxIter=10, regParam=0.01, userCol=\"reviewerID_index\", itemCol=\"asin_index\", ratingCol=\"overall\", coldStartStrategy=\"drop\")\n",
    "pipeline_mf = Pipeline(stages=[als_mf])\n",
    "model_mf = pipeline_mf.fit(train_data)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions = model_mf.transform(test_data)\n",
    "predictions = predictions.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 3.3615945234348246\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"overall\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error (MAE): 2.130344226155689\n"
     ]
    }
   ],
   "source": [
    "evaluator_mae = RegressionEvaluator(metricName=\"mae\", labelCol=\"overall\", predictionCol=\"prediction\")\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "print(\"Mean absolute error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stats = train_data.selectExpr('percentile(overall, 0.25) as q1', 'percentile(overall, 0.75) as q3', 'avg(overall) as mean', 'stddev(overall) as std_dev').collect()[0]\n",
    "iqr = target_stats.q3 - target_stats.q1\n",
    "mean = target_stats.mean\n",
    "std_dev = target_stats.std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interquartile Range (IQR): 2.0\n",
      "Mean: 4.013246800002571\n",
      "Standard Deviation: 1.278798577842928\n"
     ]
    }
   ],
   "source": [
    "print(\"Interquartile Range (IQR):\", iqr)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where you want to save the model\n",
    "path = \"file:///home/hashim/Downloads/RegModel\"\n",
    "\n",
    "# Save the model in the Hadoop file system\n",
    "model_mf.save(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE: The standard deviation of the ratings is 1.278798577842928, and the RMSE is 3.3615945234348246. Since the RMSE is higher than the standard deviation, it indicates that the model's predictions have some level of error. \n",
    "\n",
    "MAE: The mean absolute error (MAE) is 2.130344226155689. The IQR (Interquartile Range) is 2.0, which gives us an idea about the spread of the ratings. Since the MAE is close to the IQR, it suggests that, on average, the model's predictions are within a reasonable range of the actual ratings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
