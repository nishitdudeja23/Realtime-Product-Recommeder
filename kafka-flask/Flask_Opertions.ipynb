{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/dante/spark-3.4.0-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/dante/.ivy2/cache\n",
      "The jars for the packages stored in: /home/dante/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c33b800e-70ef-4add-9dda-e4e6bcc3796c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 180ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c33b800e-70ef-4add-9dda-e4e6bcc3796c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/10ms)\n",
      "23/05/12 15:27:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "spark=SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"FT\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/\") \\\n",
    "    .config(\"spark.driver.memory\",\"12g\")\\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"file:///home/dante/Desktop/Big_data/model_mf\"\n",
    "from pyspark.ml import PipelineModel\n",
    "# Load the saved model\n",
    "loaded_model_mf = PipelineModel.load(path)\n",
    "csv_path = \"file:///home/dante/Desktop/Big_data/id.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"asin\", DoubleType(), nullable=True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(csv_path, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the user to enter a double value\n",
    "user_input = input(\"Enter a double value: \")\n",
    "\n",
    "# Convert the user input to a double\n",
    "value = float(user_input)\n",
    "\n",
    "final_df = df.withColumn(\"reviewer_id\", lit(value))\n",
    "df = final_df.select(\"reviewer_id\", \"asin\")\n",
    "final_df = df.withColumnRenamed(\"reviewer_id\", \"reviewerID_index\").withColumnRenamed(\"asin\", \"asin_index\")\n",
    "user_predictions = loaded_model_mf.transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/12 15:32:42 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 14.0\n",
      " Schema: asin\n",
      "Expected: asin but found: 14.0\n",
      "CSV file: file:///home/dante/Desktop/Big_data/id.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|asin_index|\n",
      "+----------+\n",
      "|     236.0|\n",
      "|    4288.0|\n",
      "|   36436.0|\n",
      "|   40107.0|\n",
      "|     545.0|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 39722)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/dante/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/dante/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/dante/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/dante/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Sort the user_predictions DataFrame by the predictions column in descending order\n",
    "sorted_predictions = user_predictions.orderBy(desc(\"prediction\"))\n",
    "\n",
    "# Select the top 5 rows and show the asin_index column\n",
    "top_5_asin = sorted_predictions.select(\"asin_index\").limit(5)\n",
    "top_5_asin.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
